{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Obiettivo__\n",
    "\n",
    "Estrarre alcuni metadati relativi ai file PDF contenuti nella cartella _articles/_, mediante l'impiego di apposite librerie per la gestione e manipolazione di file PDF.\n",
    "\n",
    "I dati estratti sono suddivisi in:\n",
    "- __DOI__, _Digital Object Identifier_, identificativo univoco di risorse digitali\n",
    "- __Title__, titolo dell'articolo scientifico \n",
    "- __Author__, autore/autori partecipanti alla stesura dell'articolo preso in considerazione\n",
    "- __Abstract__, riassunto di un documento, privo di interpretazioni o critiche\n",
    "\n",
    "_DOI_ è l'acronimo di __Digital Object Identifier__, identificativo univoco di risorse digitali. Per poter apprendere tale informazione è necessario utilizzare la _REST API_ fornita da __CrossRef__; CrossRef è un'infrastuttura digitale dedita alle memorizzazione di tutti gli articoli posti in ambito accademico. Di seguito, si percepisce l'importanza del titolo e dell'autore per ogni PDF, affinchè interrogando la _Application Programming Interface_ sia possibile ottenere il DOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "target_word = \"abstract\"\n",
    "pdf_path = \"../articles/\"\n",
    "openai_api_key = os.getenv(\"OPENAI_KEY\")\n",
    "columns_section_dataframe = [\"Abstract\", \"Introduction\"]\n",
    "columns_metadata_dataframe = [\"DOI\", \"Title\", \"Authors?\"]\n",
    "crossref_url = \"https://api.crossref.org/works/10.1037/0003-066X.59.1.29/agency\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tre classi riportate sono utilizzate rispettivamente per:\n",
    "- __ScannedText__, contiene sezioni del file PDF scannerizzato, dove _start_index_ ed _end_index_ rappresentano l'indice della linea in cui compaiono le _key word_ ricercate, mentre _text_ è una variabile _str_ utilizzata per memorizzare la scansione da immagine a stringa effettuata tramite _pytesseract_\n",
    "- __ScannedSection__, possiede le sezioni rilevanti di ciascun file presente all'interno della directory. Per ora l'analisi si limita ai paragrafi _Abstract_ e _Introduction_\n",
    "- __Metadata__, ciascun oggetto istanziato della classe rappresenta i metadati ottenuti di ogni singolo file posto all'interno della cartella _articles/_\n",
    "- __AgentExtractor__, implementata per realizzare una __chain__ secondo le direttive della libreria __LangChain__. Utilizzata per poter estrarre il _titolo_ e gli _autori_ degli articoli privi di _metadati_ già presenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScannedText:\n",
    "    def __init__(self, start_index, end_index, text):\n",
    "        self.start_index = start_index\n",
    "        self.end_index = end_index\n",
    "        self.text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class ScannedSection:\n",
    "    def __init__(self, path, abstract, introduction):\n",
    "        self.path = path\n",
    "        self.abstract = abstract\n",
    "        self.introduction = introduction\n",
    "\n",
    "    def get_dict(self) -> Dict[str, Dict[str, str]]:\n",
    "        return {\n",
    "            self.path: {\n",
    "                \"Abstract\": self.abstract,\n",
    "                \"Introduction\": self.introduction\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metadata:\n",
    "    def __init__(self, DOI=None, path=None, title=None, author=None, abstract=None):\n",
    "        self.DOI = DOI\n",
    "        self.path = path\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.abstract = abstract\n",
    "\n",
    "    def get_dict(self) -> Dict[str, Dict[str, str]]:\n",
    "        return {\n",
    "            self.path: {\n",
    "                \"DOI\": self.DOI,\n",
    "                \"Title\": self.title,\n",
    "                \"Author\": self.author,\n",
    "                \"Abstract\": self.abstract\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "class AgentExtractor:\n",
    "    def __init__(self):\n",
    "        system_message = \"\"\"\n",
    "            You are an assistant in charge to extract the title and the authors' list of scientific paper.\n",
    "\n",
    "            You must extract at least the title and the authors, any other information is not requested. Return all the\n",
    "            fields required like an unique string; all the fields must be separated by a comma.\n",
    "\n",
    "            Example of an extraction:\n",
    "            Title, First Author, Second Author, ..., Last Author\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate(\n",
    "            [\n",
    "                (\"system\", system_message),\n",
    "                (\"human\", \"Scientific paper, which will be used to extract the title and the authors' list. \\n {text}\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            api_key=openai_api_key,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        self.agent = prompt | llm\n",
    "\n",
    "    def get_agent(self):\n",
    "        return self.agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pipeline è composta da due fasi principali:\n",
    "- __Estrazione__ dei metadati\n",
    "- __Elaborazione__ dei metadati ottenuti\n",
    "\n",
    "Le librerie utilizzate per l'acquisizione dei metadati si fondano a loro volta sulla libreria __pdfMiner__, pertanto riescono ad estrapolare le informazioni circoscritte dagli stessi file. Perciò tutte le librerie riportate all'interno del codice si equivalgono, si osservi lo snippet in cui sono dichiarate _pymupdf_, _pdfplumber_ e _PdfReader_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_path_pdf_files(pdf_path: str) -> List[str]:\n",
    "    path_files = []\n",
    "\n",
    "    for path in os.listdir(pdf_path):\n",
    "        path_files.append(pdf_path + path)\n",
    "    \n",
    "    return path_files\n",
    "\n",
    "pdf_paths = get_path_pdf_files(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pytesseract\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def convert_file_to_images(path: str) -> List:\n",
    "    try:\n",
    "        return convert_from_path(path)[:3]\n",
    "    except Exception as e:\n",
    "        print(\"Error during conversion from file to image:\", e)\n",
    "\n",
    "def scan_file_to_text(paths: List[str]) -> Dict[str, str]:\n",
    "    dict_texts: Dict[str, str] = {}\n",
    "\n",
    "    for path in paths:\n",
    "        images = convert_file_to_images(path)\n",
    "\n",
    "        text = \"\"\n",
    "        try:\n",
    "            for image in images:\n",
    "                text = text + pytesseract.image_to_string(image)\n",
    "\n",
    "            dict_texts[path] = text\n",
    "        except Exception as e:\n",
    "            print(\"Error during conversion from image to text:\", e) \n",
    "        \n",
    "    return dict_texts\n",
    "\n",
    "dict_scanned_text = scan_file_to_text(pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../articles/16DavidNetanyahuWolf.pdf: (start_21, end_35)\n",
      "../articles/90GeorgeSchaeffer.pdf: (start_10, end_30)\n",
      "../articles/91FeldmannMysliwietzMonien.pdf: (start_8, end_20)\n",
      "../articles/83CondonThompson.pdf: (start_-1, end_-1)\n",
      "../articles/07Beal.pdf: (start_4, end_14)\n",
      "../articles/09CiancariniFavini 1.pdf: (start_7, end_40)\n",
      "../articles/ICGA_J_34_2_HHB_Zugzwangs_in_Chess_Studies.pdf: (start_41, end_51)\n",
      "../articles/76Panek.pdf: (start_-1, end_-1)\n",
      "../articles/19Kamlish.pdf: (start_10, end_-1)\n",
      "../articles/96Brockington.pdf: (start_-1, end_5)\n",
      "\n",
      "\n",
      "../articles/16DavidNetanyahuWolf.pdf: (start_35, end_58)\n",
      "../articles/90GeorgeSchaeffer.pdf: (start_30, end_99)\n",
      "../articles/91FeldmannMysliwietzMonien.pdf: (start_20, end_105)\n",
      "../articles/83CondonThompson.pdf: (start_-1, end_23)\n",
      "../articles/07Beal.pdf: (start_14, end_112)\n",
      "../articles/09CiancariniFavini 1.pdf: (start_40, end_99)\n",
      "../articles/ICGA_J_34_2_HHB_Zugzwangs_in_Chess_Studies.pdf: (start_51, end_101)\n",
      "../articles/76Panek.pdf: (start_-1, end_109)\n",
      "../articles/19Kamlish.pdf: (start_-1, end_-1)\n",
      "../articles/96Brockington.pdf: (start_5, end_30)\n"
     ]
    }
   ],
   "source": [
    "def get_target_word_index(expression: str, text: str) -> int:\n",
    "    count_line = 0\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    for line in lines:\n",
    "        if re.search(expression, line.lower()) is not None:\n",
    "            return count_line\n",
    "        \n",
    "        count_line += 1\n",
    "    \n",
    "    return -1\n",
    "\n",
    "def detect_target_line(start_word: str, end_word: str, dict: Dict[str, str]) -> Dict[str, ScannedText]:\n",
    "    dict_lines: Dict[str, ScannedText] = {}\n",
    "\n",
    "    for key, value in dict.items():\n",
    "        dict_lines[key] = ScannedText(get_target_word_index(start_word, value.lower()), get_target_word_index(end_word, value.lower()), value)\n",
    "\n",
    "    return dict_lines\n",
    "\n",
    "abstracts = detect_target_line(r\"^abstract\", r\"1?\\.?\\s* [Ii]ntroduction\", dict_scanned_text)\n",
    "introductions = detect_target_line(r\"1?\\.?\\s* [Ii]ntroduction\", r\"^2.?\\W\", dict_scanned_text)\n",
    "\n",
    "def stamp_found_indexes(_dict: Dict[str, ScannedText]):\n",
    "    for key, value in _dict.items():\n",
    "        print(key + \": \" + \"(start_\" + str(value.start_index) + \", end_\" +  str(value.end_index) + \")\")\n",
    "\n",
    "stamp_found_indexes(abstracts)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "stamp_found_indexes(introductions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 2 65536 (offset 0)\n",
      "Ignoring wrong pointing object 16 65536 (offset 0)\n",
      "Ignoring wrong pointing object 49 65536 (offset 0)\n",
      "Ignoring wrong pointing object 59 65536 (offset 0)\n",
      "Ignoring wrong pointing object 63 65536 (offset 0)\n",
      "Ignoring wrong pointing object 72 65536 (offset 0)\n",
      "Ignoring wrong pointing object 76 65536 (offset 0)\n",
      "Ignoring wrong pointing object 86 65536 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "import pdfplumber\n",
    "\n",
    "from typing import Optional\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def get_title_from_dicts(pdfreader: str, pymupdf: Dict[str, str], pdfplumber: dict[str, str]) -> Optional[str]:\n",
    "    return (pdfreader or pymupdf.get(\"title\") or pdfplumber.get(\"title\") or None)\n",
    "\n",
    "def get_author_from_dicts(pdfreader: str, pymupdf: Dict[str, str], pdfplumber: dict[str, str]) -> Optional[str]:\n",
    "    return (pdfreader or pymupdf.get(\"author\") or pdfplumber.get(\"author\") or None)\n",
    "\n",
    "def extract_title_and_author(i: int, len: int, paths: list[str], list_metadata: List[Metadata] = []) -> List[Metadata]:\n",
    "    if len == 0:\n",
    "        return list_metadata\n",
    "    else:\n",
    "        _pdfreader = PdfReader(paths[i]).metadata\n",
    "        _pymupdf = pymupdf.open(paths[i]).metadata\n",
    "        _pdfplumber = pdfplumber.open(paths[i]).metadata\n",
    "\n",
    "        title = get_title_from_dicts(_pdfreader.title, _pymupdf, _pdfplumber)\n",
    "        author = get_author_from_dicts(_pdfreader.author, _pymupdf, _pdfplumber)\n",
    "\n",
    "        list_metadata.append(Metadata(path=paths[i], title=title, author=author))\n",
    "\n",
    "        extract_title_and_author(i + 1, len - 1, paths)\n",
    "        return list_metadata\n",
    "\n",
    "list_metadata = extract_title_and_author(0, len(pdf_paths), pdf_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elenco di articoli a cui manca il titolo e autore:\n",
    "- 09CiancariniFavini\n",
    "- 16DavidNetanyahuWolf\n",
    "- 19Kamlish\n",
    "- ICGA_J_34_2_HHB_Zugzwangs_in_Chess_Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe implement a method to check if the fields extracted are correct\n",
    "\n",
    "def get_authors_name(i: int, len: int, authors: List[str], field = \"\") -> str:\n",
    "    if len == 1:\n",
    "        return field + authors[i]\n",
    "    else:\n",
    "        return get_authors_name(i + 1, len - 1, authors, field + authors[i] + \" and \")\n",
    "    \n",
    "extractor = AgentExtractor()\n",
    "agent_extractor = extractor.get_agent()\n",
    "\n",
    "for metadata in list_metadata:\n",
    "    if None in (metadata.title, metadata.author):\n",
    "        metadata.title = \"\"\n",
    "        metadata.author = \"\"\n",
    "\n",
    "        answer = agent_extractor.invoke(dict_scanned_text.get(metadata.path)).content\n",
    "        fields = answer.split(\", \")\n",
    "\n",
    "        metadata.title = fields[0]\n",
    "        metadata.author = get_authors_name(0, len(fields[1:]), fields[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def extract_section_lines(start_index: int, end_index: int, text: str) -> str:\n",
    "    abstract = \"\"\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    for line in islice(lines, start_index, end_index):\n",
    "        abstract += line + \"\\n\"\n",
    "\n",
    "    return abstract\n",
    "\n",
    "def extract_section(_dict: Dict[str, ScannedText]) -> Dict[str, Optional[str]]:\n",
    "    dict_section: Dict[str, str] = {}\n",
    "\n",
    "    for key, value in _dict.items():\n",
    "        if value.start_index > -1 and value.end_index > -1:\n",
    "            text = extract_section_lines(value.start_index, value.end_index, value.text)\n",
    "        else:\n",
    "            text = None\n",
    "\n",
    "        dict_section[key] = text\n",
    "\n",
    "    return dict_section\n",
    "\n",
    "dict_abstracts = extract_section(abstracts)\n",
    "dict_introductions = extract_section(introductions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I metodi sviluppati, per l'estrazione dell'_Abstract oppure dell'_Introduction_, presentano alcune problematiche soprattutto in ottica del formato del file analizzato. Infatti, documenti che dispongono il testo in due colonne distinte sono più difficili da convertire in un formato idoneo alla manipolazione testuale, ad esempio ciò avviene per il PDF _09CiancariniFavini_. Tuttavia, è presente un ulteriore file che possiede un formato simile, ossia _19Kamlish_, da cui è possibile estrarre integralmente le sezioni citate.\n",
    "\n",
    "Pertanto, possono essere assecondati due approcci sostitutivi a quanto descritto, tra cui:\n",
    "- __CrossRef__, interrogare la _REST API_ per ottenere l'_Abstract_ tramite il _Digital Object Identifier_\n",
    "- __OCR__, implementazione di un differente _Optical Character Recognition_, oltre alla libreria _pytesseract_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "from crossref.restful import Works\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(title_extracted: str, title_crossref: str) -> float:\n",
    "    return SequenceMatcher(None, title_extracted, title_crossref).ratio()\n",
    "\n",
    "work = Works()\n",
    "\n",
    "def send_crossref_request(title: str, author: str) -> str:\n",
    "    url_request = work.query(bibliographic=title, author=author).url\n",
    "\n",
    "    # Delayed request to CrossRef Rest API how asked by the same library\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url_request)\n",
    "\n",
    "        match response.status_code:\n",
    "            case 200:\n",
    "                content = response.json()\n",
    "                message = content.get(\"message\")\n",
    "\n",
    "                items = message.get(\"items\")\n",
    "                for item in items:\n",
    "                    if similar(title, item[\"title\"][0]) > 0.5:\n",
    "                        return item[\"DOI\"]\n",
    "                                                                \n",
    "                    continue\n",
    "            case 400:\n",
    "                raise Exception(\"Error during request to CrossRef REST API: Bad Request\")\n",
    "            case _:\n",
    "                raise Exception(\"Error during request to CrossRef REST API\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during request to CrossRef REST API:\", e)\n",
    "\n",
    "for item in list_metadata:\n",
    "    if None not in (item.title, item.author):\n",
    "        doi =  send_crossref_request(item.title, item.author)\n",
    "\n",
    "        if doi is not None:\n",
    "            item.DOI = doi\n",
    "\n",
    "    item.abstract = dict_abstracts[item.path]\n",
    "    \n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scanned_section: List[ScannedSection] = []\n",
    "for key in dict_abstracts.keys():\n",
    "    list_scanned_section.append(ScannedSection(key, dict_abstracts[key], dict_introductions[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tramite la funzione _filter_, associata al metodo _query_ fornita dalla libreria __crossref__, è possibile recuperare l'_Abstract_ degli articoli presenti all'interno dell'API. Tuttavia, è stato notato che l'impiego del filtro comporta ad una risposta completamente differente rispetto alla casistica in cui sia assente. Di seguito, è presentato lo snippet di codice implementato:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <p><b>url_request = work.query(bibliographic=title, author=author).filter(has_abstract=true).url</b></p>\n",
    "</div>\n",
    "\n",
    "Gli __url__ sottostanti non ricadono nello stesso dominio individuato nelle precedenti interrogazioni all'API durante la fase di acquisizione del _DOI_. Di seguito è stato implementato un ulteriore approccio, in cui viene ricavato il _Uniform Resource Locator_ attraverso la combinazione dell'identificativo digitale e dell'estensione _.xml_, come definito dagli stessi manuntentori dell'API. Tuttavia, anche in questa casistica non è mai riportato il _tag </abstract/>_.\n",
    "\n",
    "Unico approccio risolutivo possibile potrebbe consistere in una maggiore documentazione relativa agli _Abstract_ contenuti all'interno di _CrossRef_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.crossref.org/works?filter=has-abstract%3Atrue&query.author=Joe+Condon+and+Ken+Thompson&query.bibliographic=Belle+Chess+Hardware\n",
      "https://api.crossref.org/works?filter=has-abstract%3Atrue&query.author=Leroy+Panek&query.bibliographic=%22Maelzel%27s+Chess-Player%22%2C+Poe%27s+First+Detective+Mistake\n",
      "https://api.crossref.org/works?filter=has-abstract%3Atrue&query.author=Isaac+Kamlish+and+Isaac+Bentata+Chocron+and+Nicholas+McCarthy&query.bibliographic=SentiMATE%3A+Learning+to+play+Chess+through+Natural+Language+Processing\n",
      "https://api.crossref.org/works?filter=has-abstract%3Atrue&query.author=Brockington%2C+Mark&query.bibliographic=A+taxonomy+of+parallel+game-tree+search+algorithms\n",
      "\n",
      "\n",
      "https://api.crossref.org/works/10.1007/978-1-4757-1968-0_28.xml\n",
      "https://api.crossref.org/works/10.2307/2924872.xml\n",
      "https://api.crossref.org/works/10.1007/978-94-009-5044-3_16.xml\n",
      "https://api.crossref.org/works/10.3233/icg-1996-19303.xml\n"
     ]
    }
   ],
   "source": [
    "for metadata in list_metadata:\n",
    "    if (metadata.title, metadata.author) is not None and metadata.abstract is None:\n",
    "        url_request = work.query(bibliographic=metadata.title, author=metadata.author).filter(has_abstract=\"true\").url\n",
    "        print(url_request)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for metadata in list_metadata:\n",
    "    if metadata.DOI is not None and metadata.abstract is None:\n",
    "        url_request = work.query().url + \"/\" + metadata.DOI + \".xml\"\n",
    "        print(url_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16DavidNetanyahuWolf</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90GeorgeSchaeffer</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91FeldmannMysliwietzMonien</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83CondonThompson</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07Beal</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09CiancariniFavini 1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICGA_J_34_2_HHB_Zugzwangs_in_Chess_Studies</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76Panek</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19Kamlish</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96Brockington</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              DOI  Title  Authors?\n",
       "16DavidNetanyahuWolf                         True   True      True\n",
       "90GeorgeSchaeffer                            True   True      True\n",
       "91FeldmannMysliwietzMonien                  False   True      True\n",
       "83CondonThompson                             True   True      True\n",
       "07Beal                                      False   True      True\n",
       "09CiancariniFavini 1                         True   True      True\n",
       "ICGA_J_34_2_HHB_Zugzwangs_in_Chess_Studies   True   True      True\n",
       "76Panek                                      True   True      True\n",
       "19Kamlish                                    True   True      True\n",
       "96Brockington                                True   True      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Introduction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16DavidNetanyahuWolf</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90GeorgeSchaeffer</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91FeldmannMysliwietzMonien</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83CondonThompson</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07Beal</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09CiancariniFavini 1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICGA_J_34_2_HHB_Zugzwangs_in_Chess_Studies</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76Panek</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19Kamlish</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96Brockington</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  Introduction\n",
       "16DavidNetanyahuWolf                            True          True\n",
       "90GeorgeSchaeffer                               True          True\n",
       "91FeldmannMysliwietzMonien                      True          True\n",
       "83CondonThompson                               False         False\n",
       "07Beal                                          True          True\n",
       "09CiancariniFavini 1                            True          True\n",
       "ICGA_J_34_2_HHB_Zugzwangs_in_Chess_Studies      True          True\n",
       "76Panek                                        False         False\n",
       "19Kamlish                                      False         False\n",
       "96Brockington                                  False          True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pandas\n",
    "\n",
    "def has_attributes_metadata(item: object) -> List[bool]:\n",
    "    return [item.DOI is not None, hasattr(item, \"title\"), hasattr(item, \"author\")]\n",
    "\n",
    "list_metadata_json = []\n",
    "for metadata in list_metadata:\n",
    "    list_metadata_json.append(metadata.get_dict())\n",
    "\n",
    "dict_metadata_dataframe: Dict[str, List[bool]] = {}  \n",
    "for metadata in list_metadata:\n",
    "    dict_metadata_dataframe[metadata.path[12:][:-4]] = [metadata.DOI is not None, metadata.title is not None, metadata.author is not None]\n",
    "\n",
    "list_scanned_section_json = []\n",
    "for section in list_scanned_section:\n",
    "    list_scanned_section_json.append(section.get_dict())\n",
    "\n",
    "dict_scanned_section_dataframe: Dict[str, List[bool]] = {}  \n",
    "for section in list_scanned_section:\n",
    "    dict_scanned_section_dataframe[section.path[12:][:-4]] = [section.abstract is not None, section.introduction is not None]\n",
    "\n",
    "with open(\"../json/extraction/metadata_partial.json\", \"a\") as file:\n",
    "    json.dump(list_metadata_json, file, indent=2)\n",
    "\n",
    "# Summary about the data retrieved\n",
    "dataframe_metadata = pandas.DataFrame.from_dict(dict_metadata_dataframe, orient=\"index\", columns=columns_metadata_dataframe)\n",
    "display(dataframe_metadata)\n",
    "\n",
    "dataframe_scanned_section = pandas.DataFrame.from_dict(dict_scanned_section_dataframe, orient=\"index\", columns=columns_section_dataframe,)\n",
    "display(dataframe_scanned_section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilizzo di una __chain__, secondo le regole implementative espresse dalla libreria __LangChain__, ha permesso l'estrazione del _titolo_ e dell'_autore_ per tutti i file non in possesso dei _metadati_ ricercati. Tuttavia, ciò ha garantito l'estrazione delle informazioni per un totale di 8 file su 10, pertanto un rapporto che va oltre alla media, si osservi il risultato ottenuto in seguito al _run all_ in _json/metadata/metadata.json_. \n",
    "\n",
    "Migliorie possono essere adottate per quanto concerne la _Regular Expression_ utilizzata per estrapolare l'_Introduction_. Tuttavia, sono state acquisite un totale di 7 _Introduction_ sui 10 file in possesso, come visualizzabile all'interno del _json_ posto in _json/metadata/section.json_.\n",
    "\n",
    "\n",
    "Piccoli accorgimenti potrebbero essere utilizzati per i seguenti articoli, di cui non si ha il _DOI_:\n",
    "- _91FeldmannMysliwietzMonien_\n",
    "- _07Beal_\n",
    "- _19Kamlish_\n",
    "\n",
    "Il file denominato _91FeldmannMysliwietzMonien_ presenta l'_identificativo digitale_ nella sezione _reference/unstructured_ della risposta ricevuta dalla _REST API_.\n",
    "Gli ultimi due riportati sono stati cercati manualmente all'interno dell'API di _CrossRef_; infatti non è stata delineata una determinata persistenza dei dati, anzi gli articoli scientifici non compaiono tra quelli proposti."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
