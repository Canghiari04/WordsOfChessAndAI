{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Obiettivo__\n",
    "\n",
    "Estrarre alcuni metadati relativi ai file PDF contenuti nella cartella _articlesGrobid/_, mediante l'impiego di apposite librerie.\n",
    "\n",
    "Nuovamente, i dati estratti si suddividono in:\n",
    "- __DOI__, _Digital Object Identifier_, identificativo univoco di risorse digitali\n",
    "- __Title__, titolo dell'articolo scientifico\n",
    "- __Authors__, autore/autori partecipanti alla stesura del paper preso in considerazione\n",
    "- __Abstract__, piccolo riassunto del documento, privo di interpretazioni o critiche\n",
    "\n",
    "Il notebook ricalca lo stesso obiettivo delineato in __metadata.ipynb__, ma prendendo in esame un bacino molto più elevato rispetto ai dieci file contenuti nella cartella _articles/_.\n",
    "Proprio per questa principale ragione, è stata adeguata una specifica libreria denominata __Grobid__, acronimo di _Generation Of Bibliographic Data_.\n",
    "\n",
    "_Grobid_ è una libreria di _Machine Learning_, il cui scopo consiste nell'estrazione e conversione di un file PDF in un _formato struttutato XML_. Le funzionalità sviluppate permettono di acquisire un insieme di dati già confezionato, disposti secondo l'ordine gerarchico espresso dal _markup language_ in questione. \n",
    "\n",
    "Infatti, come presentato negli snippet di codice successivi, è stata adeguata la libreria _BeatifulSoupe_ affinchè il risultato ottenuto da _Grobid_, memorizzato all'interno di un'apposita directory, fosse convertito in un formato idoneo al linguaggio di programmazione utilizzato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../../articles/articlesCiancarini/\"\n",
    "\n",
    "input_dir = \"../../articles/articlesGrobid/\"\n",
    "output_dir = \"../../articles/articlesTEI/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le due classi presentate sono utilizzate rispettivamente per:\n",
    "- __Author__, classe rappresentativa di tutti gli _autori_ degli articoli scientifici\n",
    "- __Metadata__, ciascun oggetto istanziato della classe rappresenta i _metadati_ ottenuti di ogni singolo file posto all'interno della cartella _articlesGrobid/_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author:\n",
    "    def __init__(self, forename: str, surname: str):\n",
    "        self.forename = forename\n",
    "        self.surname = surname\n",
    "\n",
    "    def to_unique(self) -> str | None:\n",
    "        return self.forename + \" \" + self.surname or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class Metadata:\n",
    "    def __init__(self, DOI: str, path: str, title: str, author: List[Author], keyword: List[str], abstract: str, introduction: str):\n",
    "        self.DOI = DOI\n",
    "        self.path = path\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.keyword = keyword\n",
    "        self.abstract = abstract\n",
    "        self.introduction = introduction\n",
    "\n",
    "    def get_dict(self) -> Dict[str, Dict[str, str | List[Author] | None]]:\n",
    "        return {\n",
    "            self.path: {\n",
    "                \"DOI\": self.DOI,\n",
    "                \"Title\": self.title,\n",
    "                \"Author\": [item.to_unique() for item in self.author],\n",
    "                \"Keyword\": self.keyword,\n",
    "                \"Abstract\": self.abstract,\n",
    "                \"Introduction\": self.introduction\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Took a certain range of PDF from the articlesCiancarini to articlesGrobid\n",
    "def define_grobid_dir(main_path: str):\n",
    "    for path in os.listdir(main_path):\n",
    "        year = int(path[:2])\n",
    "\n",
    "        try:\n",
    "            if year in range(0, 100) and path.endswith(\".pdf\"):\n",
    "                shutil.copy(main_path + path, input_dir)\n",
    "        except ValueError as e:\n",
    "            continue\n",
    "\n",
    "if not os.path.exists(input_dir):\n",
    "    os.makedirs(input_dir)\n",
    "    define_grobid_dir(pdf_path)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di eseguire lo snippet di codice seguente, è necessario abilitare la _Web API_ di _Grobid_. Ciò può avvenire in due modi, suddivisi in:  \n",
    "- Installazione di _Grobid_ localmente, in cui è richiesta la previa presenza del _JDK (Java Development Kit)_ e di _Gradle_\n",
    "- Utilizzo di _Docker_ tramite l'_image_ fornita dalla documentazione, necessaria per realizzare il _container_ associato, ossia l'istanza eseguibile della stessa immagine\n",
    "\n",
    "La scelta progettuale è ricaduta su _Docker_, data l'estrema semplicità garantita dal tool. Di seguito, è riportato il comando necessario per abilitare la _Web API_:\n",
    "<div align=\"center\">\n",
    "    <p><b>docker run --rm --init --ulimit core=0 -p 8070:8070 grobid/grobid:0.8.1</b></p>\n",
    "</div>\n",
    "\n",
    "Infine, attraverso un qualsiasi _browser_, è possibile accertarsi se il servizio operi correttamente, accedendo alla pagina dedicata alla _console_ di _Grobid_, esposta tramite _localhost:8070_. La risposta ricevuta dal servizio consiste in un insieme di file _XML_, i quali sono memorizzati all'interno della cartella _TEI/_.\n",
    "\n",
    "A volte _Grobid_ potrebbe restituire _error 503_, ossia un errore interno del server, in questo caso conviene attendere qualche minuto prima di tentare nuovamente di interagire con il servizio, dato che tutti i thread messi a disposizione sono già utilizzati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from grobid_client.grobid_client import GrobidClient\n",
    "\n",
    "try:\n",
    "    grobid_client = GrobidClient(config_path=\"../../json/grobid/config.json\")\n",
    "    grobid_client.process(\"processFulltextDocument\", input_dir, output_dir, n=10)        \n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(\"Connection error during Grobid processing: \", e)\n",
    "except Exception as e:\n",
    "    print(\"Error during Grobid processing: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_xml_paths() -> List[str]:\n",
    "    path_files = []\n",
    "\n",
    "    for path in os.listdir(output_dir):\n",
    "        path_files.append(path)\n",
    "\n",
    "    return path_files\n",
    "\n",
    "xml_paths = define_xml_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Extracting content from XML, after defining a list of progressive tags\n",
    "def extract_content(soup: BeautifulSoup, tags: List[str]) -> str | None:\n",
    "    try:\n",
    "        element = soup.find(tags[0])\n",
    "\n",
    "        for tag in tags[1:]:\n",
    "            element = element.find(tag)\n",
    "\n",
    "        return element.contents[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "# Function used to detect authors' forename and username by RegEx\n",
    "def extract_name(expression: str, persName) -> str | None:\n",
    "    try:\n",
    "        match = re.search(expression, str(persName))\n",
    "\n",
    "        return match.group(1)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# BeatifulSoup consists in a data structure representing a parsed XML file\n",
    "def extract_field_from_xml(path_files: List[str]) -> List[Metadata]:\n",
    "    list_authors: List[Author] = []\n",
    "    list_metadata: List[Metadata] = []\n",
    "    \n",
    "    for path in sorted(path_files):\n",
    "        try: \n",
    "            if path.endswith(\".xml\"):\n",
    "                with open(output_dir + path, \"r\") as content:\n",
    "                    xml = content.read()\n",
    "\n",
    "                soup = BeautifulSoup(xml, \"xml\")\n",
    "                \n",
    "                title = extract_content(soup, [\"titleStmt\", \"title\"])\n",
    "                abstract = extract_content(soup, [\"profileDesc\", \"abstract\", \"p\"])\n",
    "        \n",
    "                try:\n",
    "                    introduction = extract_content(soup, [\"body\", \"div\", \"p\"])\n",
    "                except Exception:\n",
    "                    introduction = None\n",
    "\n",
    "                try: \n",
    "                    persNames = soup.find(\"sourceDesc\").find(\"biblStruct\").find(\"analytic\").find_all(\"persName\")\n",
    "                except Exception:\n",
    "                    persNames = None\n",
    "\n",
    "                if persNames is not None:\n",
    "                    for persName in persNames:\n",
    "                        forename = extract_name(r\"<forename\\stype=\\\"first\\\">(.*?)<\\/forename>\", persName)\n",
    "                        surname = extract_name(r\"<surname>(.*?)<\\/surname>\", persName)\n",
    "                        \n",
    "                        if None not in (forename, surname):\n",
    "                            author = Author(forename, surname)\n",
    "                            list_authors.append(author)\n",
    "\n",
    "                list_metadata.append(Metadata(DOI=None, path=path, title=title, author=list_authors, keyword=None, abstract=abstract, introduction=introduction))\n",
    "                list_authors = []\n",
    "                \n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(\"Error during parsing \", path,\" file: \", e)\n",
    "\n",
    "    return list_metadata\n",
    "\n",
    "list_metadata = extract_field_from_xml(xml_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonostante l'applicazione di _Grobid_, alcuni PDF sono privi di _metadati_. Pertanto, affinchè l'analisi condotta possa essere estesa ad un numero sempre più vasto di file,\n",
    "è stata implementata la libreria _pypdf_; quest'ultima, permette di manipolare ogni singola pagina che componga il PDF. A tal proposito, sono acquisite ulteriori informazioni disponibili tramite il field _metadata_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def define_authors(str_author: str) -> List[Author]:\n",
    "    list_authors: List[Metadata] = []\n",
    "    try:\n",
    "        authors = re.split(r\"\\sand\\s\", str_author)\n",
    "\n",
    "        for author in authors:\n",
    "            items = author.split(\" \")\n",
    "            list_authors.append(Author(items[0], items[1]))\n",
    "\n",
    "        return list_authors\n",
    "    except Exception:\n",
    "        return list_authors\n",
    "\n",
    "def retrieve_missing_metadata(list_metadata: List[Metadata]):\n",
    "    for metadata in list_metadata:\n",
    "        if metadata.title is None or len(metadata.author) == 0:\n",
    "            pdf_path = re.search(r\"(^.*)?.grobid\", metadata.path).group(1) + \".pdf\"\n",
    "\n",
    "            try:\n",
    "                _pdfreader = PdfReader(input_dir + pdf_path).metadata\n",
    "            except Exception:\n",
    "                pass\n",
    "             \n",
    "            title = _pdfreader.title or None\n",
    "            author = _pdfreader.author or []\n",
    "\n",
    "            metadata.title = title\n",
    "            metadata.path = pdf_path\n",
    "            metadata.author = define_authors(author)\n",
    "\n",
    "retrieve_missing_metadata(list_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il tipo di alcuni metadati potrebbe differire rispetto al _type str_, pertanto è attuato un check che vada a sovrascrivere il contenuto di alcuni field errati. Ciò avviene per ovviare ad eccezioni di conversione della lista di metadati estratti in formato json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fields = [\"abstract\", \"introduction\"]\n",
    "\n",
    "for metadata in list_metadata:\n",
    "    for field in list_fields:\n",
    "        if not isinstance (getattr(metadata, field, None) , str):\n",
    "            setattr(metadata, field, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import langdetect\n",
    "\n",
    "from typing import Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Extracting candidate keywords from the abstract\n",
    "def extract_candidates_keyword(text: str) -> numpy.ndarray:\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), stop_words=\"english\").fit([text])\n",
    "\n",
    "    return vectorizer.get_feature_names_out()\n",
    "\n",
    "# Converting string type to numerical type \n",
    "def define_embeddings(text: str, candidates: numpy.ndarray) -> Tuple[numpy.ndarray, list]:\n",
    "    model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n",
    "\n",
    "    text_embeddings = model.encode([text])\n",
    "    candidates_embeddings = model.encode(candidates)\n",
    "\n",
    "    return (text_embeddings, candidates_embeddings)\n",
    "\n",
    "for metadata in list_metadata:\n",
    "    try:\n",
    "        if metadata.abstract is not None and langdetect.detect(metadata.abstract) == \"en\":\n",
    "            candidates = extract_candidates_keyword(metadata.abstract)\n",
    "            paper_embeddings, candidates_embeddings = define_embeddings(metadata.abstract, candidates)\n",
    "\n",
    "            distances = cosine_similarity(paper_embeddings, candidates_embeddings)\n",
    "            keywords = [candidates[index] for index in distances.argsort()[0][-5:]]\n",
    "\n",
    "            metadata.keyword = keywords\n",
    "    except Exception: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper contains null in title or/and author: 00Davidson Opponent modeling in Poker.pdf\n",
      "Error during request to CrossRef REST API: 'title'\n",
      "Error during request to CrossRef REST API: 'title'\n",
      "Error during request to CrossRef REST API: 'title'\n",
      "Paper contains null in title or/and author: 07Ross.pdf\n",
      "Paper contains null in title or/and author: 08DelliPriscolli.pdf\n",
      "Error during request to CrossRef REST API: 'title'\n",
      "Error during request to CrossRef REST API: 'title'\n",
      "Paper contains null in title or/and author: 09Nippold.pdf\n",
      "Paper contains null in title or/and author: 10Jerz.pdf\n",
      "Paper contains null in title or/and author: 11.Reingold.Sheridan.EyeMovementsandVisualExpertiseinChessandMediciner.pdf\n",
      "Paper contains null in title or/and author: 12Hauptmann - Thomas Eakinss The Chess Players Replayed The Metropolitan Museum J 47.pdf\n",
      "Paper contains null in title or/and author: 12Linden.pdf\n",
      "Paper contains null in title or/and author: 13Jerz.pdf\n",
      "Paper contains null in title or/and author: 13Rappen - Kindergartenschach.pdf\n",
      "Paper contains null in title or/and author: 14Nicotera.pdf\n",
      "Paper contains null in title or/and author: 14Riis.pdf\n",
      "Paper contains null in title or/and author: 15Issac.pdf\n",
      "Paper contains null in title or/and author: 16Chess position evaluation with convolutional neural network in Julia .pdf\n",
      "Paper contains null in title or/and author: 16O'Hara Building a Chess Web Application with an Agile Development Team.pdf\n",
      "Paper contains null in title or/and author: 17Cowley.pdf\n",
      "Paper contains null in title or/and author: 53Schwalb.pdf\n",
      "Paper contains null in title or/and author: 59Finot.pdf\n",
      "Paper contains null in title or/and author: 60Reider.pdf\n",
      "Paper contains null in title or/and author: 62Ginzburg.pdf\n",
      "Paper contains null in title or/and author: 64NewellSimon.pdf\n",
      "Paper contains null in title or/and author: 67Kjetsaa.pdf\n",
      "Paper contains null in title or/and author: 71Sargent.pdf\n",
      "Paper contains null in title or/and author: 73Zobrist.pdf\n",
      "Paper contains null in title or/and author: 75BuckoltzWetherell.pdf\n",
      "Paper contains null in title or/and author: 79Weinberg.pdf\n",
      "Paper contains null in title or/and author: 84Cogswell.pdf\n",
      "Paper contains null in title or/and author: 88White Querg chess.pdf\n",
      "Paper contains null in title or/and author: 90HsuAnanthamaran.pdf\n",
      "Paper contains null in title or/and author: 92Nottingham.pdf\n",
      "Paper contains null in title or/and author: 92Pell Metagame.pdf\n",
      "Error during request to CrossRef REST API: 'title'\n",
      "Error during request to CrossRef REST API: 'title'\n",
      "Error during request to CrossRef REST API: 'title'\n",
      "Paper contains null in title or/and author: 95Stewart a.pdf\n",
      "Error during request to CrossRef REST API: 'title'\n",
      "Paper contains null in title or/and author: 96Bercovitch.pdf\n",
      "Error during request to CrossRef REST API: 'title'\n",
      "Paper contains null in title or/and author: 96Thompson.pdf\n",
      "Error during request to CrossRef REST API: 'title'\n",
      "Error during request to CrossRef REST API: 'title'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "from crossref.restful import Works\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def get_authors_string(list_authors: List[Author]) -> List[str]:\n",
    "    str_authors = \"\"\n",
    "    for i in range(0, len(list_authors)):\n",
    "        if i >= len(list_authors):\n",
    "            str_authors += list_authors[i].forename + \" \" + list_authors[i].surname\n",
    "        else:\n",
    "            str_authors += list_authors[i].forename + \" \" + list_authors[i].surname + \" \"\n",
    "\n",
    "    return str_authors\n",
    "\n",
    "def similar(str_1: str, str_2: str) -> float:\n",
    "    return SequenceMatcher(None, str_1, str_2).ratio()\n",
    "\n",
    "work = Works()\n",
    "\n",
    "def send_crossref_request(title: str, list_authors: List[str]) -> str:\n",
    "    url_request = work.query(bibliographic=title, author=get_authors_string(list_authors)).url\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url_request)\n",
    "\n",
    "        match response.status_code:\n",
    "            case 200:\n",
    "                content = response.json()\n",
    "                message = content.get(\"message\")\n",
    "\n",
    "                items = message.get(\"items\")\n",
    "                for item in items:\n",
    "                    if similar(title, item[\"title\"][0]) > 0.5:\n",
    "                        return item[\"DOI\"]\n",
    "                                                                \n",
    "                    continue\n",
    "            case 400:\n",
    "                print(\"Error during request to CrossRef REST API: Bad Request\")\n",
    "            case _:\n",
    "                print(\"Error during request to CrossRef REST API: WTH\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during request to CrossRef REST API:\", e)\n",
    "\n",
    "for metadata in list_metadata:\n",
    "    if None in (metadata.title, metadata.author):\n",
    "        print(\"Paper contains null in title or/and author:\", metadata.path)\n",
    "    else:\n",
    "        doi = send_crossref_request(metadata.title, metadata.author)\n",
    "\n",
    "        if doi is not None:\n",
    "            metadata.DOI = doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_not_found_dois = list(filter(lambda metadata: metadata.DOI is None, list_metadata))\n",
    "list_found_dois = list(filter(lambda metadata: metadata.DOI is not None, list_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sezione finale del notebook si concentra su alcuni controlli attuati per accertarsi della __bontà__ dei _Digital Object Identifier_ ricavati. Ciò avviene interrogando nuovamente la _REST API_ di _CrossRef_. Tuttavia, in questa casistica, non sono combinati il _title_ e gli _authors_ estrapolati precedentemente, attraverso apposite librerie, ma è inserito all'interno dell'_URL_ lo stesso _DOI_; in questo modo, è possibile definire il grado di similarità tra le informazioni già in possesso rispetto ai nuovi dati ottenuti dalla risposta dell'_Application Programming Interface_.\n",
    "\n",
    "Di seguito, sono riportate le operazioni principali del comportamento descritto, suddivise in:\n",
    "- __Definizione della lista__, lista contenente l'insieme di tutti i _metadati_ riferiti ai PDF contenuti nella directory _articlesGrobid/_\n",
    "- __Invio della richiesta__, richiesta riferita alla _REST API_ di _CrossRef_, da cui, qualora sia positiva la risposta, saranno ricavati i \"nuovi\" dati necessari per il confronto\n",
    "- __Definizione grado di similarità__, tramite la libreria _SequenceMatcher_ è estrapolato il _grado di similarità_ del _metadato_ analizzato, qualora dovesse essere maggiore di _0.6_, dato che si tratta di un valore decimale appartenente all'intervallo _[0, 1]_, è rimosso dalla lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small check to detect the correctness of metadata\n",
    "def define_wrong_dois(list_metadata: List[Metadata]) -> List[Metadata]:\n",
    "    # List that will contain the \"wrong\" DOIs retrieved previously\n",
    "    list_wrong_dois = list(filter(lambda metadata: metadata.DOI is not None, list_metadata))\n",
    "\n",
    "    for metadata in list_metadata:\n",
    "        if metadata.DOI is not None:\n",
    "            url_request = work.query().url + \"/\" + metadata.DOI\n",
    "        \n",
    "            try:\n",
    "                response = requests.get(url_request)\n",
    "\n",
    "                match response.status_code:\n",
    "                    case 200:\n",
    "                        content = response.json()\n",
    "\n",
    "                        try:\n",
    "                            message = content.get(\"message\")\n",
    "\n",
    "                            # Another check may concern the authors and the publication date\n",
    "                            titles = message.get(\"title\")\n",
    "                            authors = message.get(\"author\")\n",
    "                \n",
    "                            for title in titles:\n",
    "                                if similar(metadata.title, title) > 0.6:\n",
    "                                    list_wrong_dois.remove(metadata)\n",
    "                                    break\n",
    "\n",
    "                                continue\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(\"Field not found\", metadata.path)\n",
    "                            continue\n",
    "                    case 400:\n",
    "                        raise Exception(\"Error during request to CrossRef REST API: Bad Request\")\n",
    "                    case _:\n",
    "                        raise Exception(\"Error during request to CrossRef REST API: WTF\")\n",
    "            except Exception as e:\n",
    "                print(\"Error during request to CrossRef REST API:\", e)\n",
    "\n",
    "    return list_wrong_dois\n",
    "\n",
    "list_wrong_dois = define_wrong_dois(list_metadata)\n",
    "list_correct_dois = list(filter(lambda metadata: metadata in list_found_dois and metadata not in list_wrong_dois, list_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def define_list(_list: List[Metadata]) -> List:\n",
    "    list_json = []\n",
    "\n",
    "    for item in _list:\n",
    "        list_json.append(item.get_dict())\n",
    "\n",
    "    return list_json\n",
    "\n",
    "with open(\"../../json/extraction/metadata_completed.json\", \"w\") as file:\n",
    "    json.dump(define_list(list_metadata), file, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>1770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Found</th>\n",
       "      <td>1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not found</th>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           # DOI\n",
       "Total       1770\n",
       "Found       1131\n",
       "Not found    639"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Found</th>\n",
       "      <td>1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Correct</th>\n",
       "      <td>944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wrong</th>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         # DOI\n",
       "Found     1131\n",
       "Correct    944\n",
       "Wrong      187"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas \n",
    "\n",
    "dict_doi_summary = {\n",
    "    \"Total\": len(list_metadata),\n",
    "    \"Found\": len(list_found_dois),\n",
    "    \"Not found\": len(list_not_found_dois)\n",
    "}\n",
    "\n",
    "dict_doi_accuracy = {\n",
    "    \"Found\": len(list_found_dois),\n",
    "    \"Correct\": len(list_correct_dois),\n",
    "    \"Wrong\": len(list_wrong_dois)\n",
    "}\n",
    "\n",
    "df_doi_summary = pandas.DataFrame.from_dict(dict_doi_summary, orient=\"index\", columns=[\"# DOI\"])\n",
    "display(df_doi_summary)\n",
    "\n",
    "df_doi_accuracy = pandas.DataFrame.from_dict(dict_doi_accuracy, orient=\"index\", columns=[\"# DOI\"])\n",
    "display(df_doi_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
