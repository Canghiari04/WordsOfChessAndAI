L'apprendimento \textbf{auto-supervisionato} ha raggiunto notevoli traguardi in vari ambiti del Natural Language Processing \cite{DBLP:journals/corr/abs-1910-13461}. Il \textbf{self-supervised learning} è una via di mezzo tra l'approccio \textbf{supervisionato} e \textbf{non-supervisionato} poiché acquisisce in input dati non annotati e li arricchisce con etichette generate autonomamente. \vspace{7pt} \\
I modelli supervised sono in grado di ottenere ottime prestazioni per i compiti per cui sono stati allenati ma non sono in grado di adeguare una generalizzazione degli stessi. Per ricavare le conoscenze necessarie per svolgere nuovi compiti è necessario fornire continuamente nuovi insiemi di dati etichettati, tuttavia spesso non sono presenti o troppo costosi da ricavare. Da questa circostanza è nata la necessità di modelli in grado di accumunare i due aspetti, ovvero comprendere meglio la realtà mediante i dati di training. Il self-supervised è una delle tecniche più promettenti per riuscire ad approssimare tale richiesta.  \vspace{7pt} \\
Il caso d'uso principale per modelli auto-supervisionati prevede l'elaborazione del linguaggio naturale, in totale assenza di esempi etichettati. Infatti, i sistemi self-supervised sono progettati in modo tale che la classificazione delle osservazioni sia dedotta in totale assenza di informazioni previamente categorizzate. \vspace{7pt} \\  
Sebbene l'apprendimento auto-supervisionato vari a seconda dello scenario, i modelli risultanti sono generalmente addestrati seguendo due approcci:
\begin{itemize}
    \renewcommand{\labelitemi}{-}
    \item \textbf{Apprendimento auto-predittivo}. \\ L'apprendimento auto-predittivo addestra un modello affinché possa prevedere parte di un campione di osservazioni date le informazioni rimanenti. I modelli che rientrano in questa categoria sono gli \textbf{Autoencoder}. Un Autoencoder raffigura una rete neurale incaricata di effettuare due azioni successive: inizialmente codifica le variabili in ingresso ricevute, per poi attuare una decodifica volta a ripristinare i dati nella loro forma originale. Ricostituendo le informazioni, i modelli estrapolano una rappresentazione estremamente significativa, poiché permette di apprendere solo le relazioni più importanti che coesistono all'interno del dataset, agevolando in questo modo la possibilità di risolvere problemi non ancora noti.
    \item \textbf{Apprendimento contrastivo}. \\
    I modelli basati sul \textbf{contrastive learning} apprendono la capacità di distinguere coppie di dati che contengano elementi simili, basandosi esclusivamente sulle features estratte dalle osservazioni. La peculiarità di questa tecnica sta nel fatto che non richieda dati etichettati pur di ricavare tali features. I modelli ottenuti sono spesso impiegati per il riconoscimento delle immagini.
\end{itemize}
Proseguendo, è possibile affermare che la letteratura è contraddistinta da una numerosa cardinalità di modelli di intelligenza artificiale addestrati tramite un approccio auto-supervisionato, i quali hanno contribuito alla risoluzione di differenti problematiche in sviariati settori. Uno tra questi, che si è distinto per il significativo impatto nel campo NLP, è l'algoritmo di machine learning \textbf{BART}. \vspace{7pt} \\
BART è un \textbf{pre-trained denoising autoencoder}, ovvero un modello addestrato su un ampio insieme di dati corrotti, progettato per ricostruirli nel loro stato iniziale. Come suggerisce la definizione, utilizza una particolare tipologia di Autoencoder in cui i dati in ingresso non vengono solamente compressi in una rappresentazione più compatta, causando una perdita di informazioni, ma subiscono un'alterazione intenzionale, affinché non coincidano con quelli originali. \vspace{7pt} \\
Dato che si formalizza su paradigmi self-supervised, permette di eseguire task relative al linguaggio naturale per archivi di informazioni prive di etichette. Il modello si dimostra particolarmente efficacie sia per operazioni di generazione di testo ma anche per compiti di comprensione ed interpretazione, capacità che lo rendono un perfetto candidato per attività di classificazione. \vspace{7pt} \\
Tra le funzionalità offerte, consente di realizzare una classificazione testuale secondo il metodo \textbf{Zero-Shot}: forniti i dati e la lista predefinita di classi da associare, restituisce un numero reale compreso tra zero e uno per stabilire il grado di correlazione tra ciascuna osservazione e la collezione di etichette.